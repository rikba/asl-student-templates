\chapter{Controls}
\label{sec:controls}
The cascaded multirotor control with decoupled attitude and position control as depicted in figure \ref{pics:controller_sketch} is a popular structure. Both nonlinear and linear control schemes have been proposed for this. These include integral backstepping, sliding mode, geometric control, PID control, learning control, mathematical optimization or combinations of these.

Bouabdallah formulated integral backstepping and sliding mode control \cite{Bouabdallah2005,Bouabdallah2007}. Lee developed a geometric tracking control \cite{Lee2010}. Hoffmann et. al. used PID control for their STARMAC quadrotor \cite{Hoffmann2007}. Robust $H_\infty$ control gives the ability guarantee stability even with model uncertainties. E.g. Felix Bergenkamp used Gaussian learning to determine uncertainty and then used $H_\infty$ to control a quadrotor \cite{Berkenkamp2014,www:robustquad}. Model predictive control schemes become more spread in multirotor control. Tzes, Nikolakopoulos and Alexis build a full linear MPC for a quadrotor \cite{Tzes2012}. Burri developed a decoupled MPC for position control \cite{Burri2012}. 

We decide to go with the mathematical optimization techniques and will derive a LQR with integrator, a linear offset-free MPC and a nonlinear MPC with integrator for general multirotor platforms. The algorithms will then be tested in the proposed MATLAB simulator and compared.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% LQRI
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{LQRI} 
LQR is a mathematical optimization based control technique for linear control systems with quadratic cost functions. The controller is straight forward to tune, easy applicable for multiple input multiple output (MIMO), guarantees stability for the given linearized system and is optimal with respect to the system representation and cost function. Further, the gain matrix $\mathbf{K}$ can be computed offline, such that applying the control law reduces to a simple matrix multiplication.

\subsection{Problem Formulation}
Given a discrete-time linear system with infinite-time quadratic cost
\begin{align}
x(k+1) &= \mathbf{A}x(k) + \mathbf{B}u(k) \label{eq:lqr_system_x}\\
J(u(0),u(1),...,u(\infty)) &= \sum_{k=0}^\infty  x(k)^T\mathbf{Q}x(k) + u(k)^T\mathbf{R}u(k) \label{eq:lqr_system_J}
\end{align} 
with $(\mathbf{A},\mathbf{B})$ stabilizable, $(\mathbf{A},\mathbf{G})$ with $\mathbf{Q}= \mathbf{G} \mathbf{G}^T$ controllable, weighting matrices $\mathbf{R}\succ0$ positive definite and $\mathbf{Q}\succeq0$ positive semi-definite, one can find a unique, stabilizing, linear state-feedback control law $u(k) = -\mathbf{K} x(k)$, that minimizes the quadratic cost.
This law is found solving the discrete-time algebraic Riccati equation for $x$.
\begin{align}
\mathbf{A}^Tx\mathbf{A} - x - \mathbf{A}^T x \mathbf{B} \left( \mathbf{B}^T x \mathbf{B} + \mathbf{R} \right)^{-1} \mathbf{B}^T x \mathbf{A} + \mathbf{Q} = 0 \\
\mathbf{K} = \left( \mathbf{B}^T x \mathbf{B} + \mathbf{R} \right)^{-1} \mathbf{B}^T x \mathbf{A}
\end{align}

For offset-free reference tracking the system \ref{eq:lqr_system_x} can be augmented by an integrator state $e$.
\begin{align}
e(k+1) = e(k) + {T_s} \left( r(k) - \mathbf{H}x(k) \right) \label{eq:lqr_system_e}
\end{align}
$r(k)$ is the reference to be tracked, $\mathbf{H}x(k)$ is the corresponding state and $T_s$ is the sampling time.

More details on LQR/LQRI control, the solution of the algebraic Riccati equation, stability and tuning can be obtained in Anderson and Moore \cite{Anderson2007}.

\subsection{Implementation}
Having set up the linear state-space model \ref{eq:lin_sys_dt}--\ref{eq:lin_sys_y}, the LQRI formulation is almost obtained. The system has to be discretized (denoted with a bar) with respect the position control sampling time $Ts=0.01\si{\second}$ and augmented with an integrator state .We will not model the effects of wind. Here, the integrator has to compensate offsets due to wind. 

In the general equations \ref{eq:lqr_system_x}, \ref{eq:lqr_system_J}, \ref{eq:lqr_system_e} we substitute the variables from our multirotor problem derived in the modeling section \ref{sec:modeling}.
\begin{align}
\mathbf{A} & = \discrete{\mathbf{A}} \\
\mathbf{B} & = \discrete{\mathbf{B}} \\
x(k) &= \orientation{\mathbf{x}}(k) - \mathbf{x}_{\mathcal{O},ss}  \\
u(k) &= \orientation{\mathbf{u}}(k) - \mathbf{u}_{\mathcal{O},ss} \label{eq:lqri_u}\\
r(k) &= \mathbf{r}_{\mathcal{O},ref}(k) \\
\mathbf{H} &= \begin{bmatrix}
\mathbf{0}_{3\times4} & \mathbf{I}_3 & \mathbf{0}_{3\times3}
\end{bmatrix} 
\end{align}
An alternative approach could have been to linearize the system at every sample instant around the current state. However, this requires for the DARE to be solved at every instant. This is for example done at a high rate in rocket control by Cimen \cite{Cimen2008}.

For reference tracking the error term $e(k)$ has to be minimized. This means, that we have to penalize it in the cost function. Because we want a fast response, we do not penalize the other states. On the other hand, we will apply some weight to the inputs in order to fulfill the criterion $\mathbf{R}\succ0$ and bound the inputs. Hence, after some adjustment, the following weighting matrices $\mathbf{Q} \in \mathbb{R}^{13\times13}$ and $\mathbf{R} \in \mathbb{R}^{3\times3}$ have been chosen for the Firefly. 
\begin{align}
\mathbf{Q} &= \begin{bmatrix}
\mathbf{0}_{10\times10} & \mathbf{0}_{10\times3} \\
\mathbf{0}_{3\times10} & \mathbf{I}_{3} 
\end{bmatrix} \\
\mathbf{R} &= \begin{bmatrix}
0.01 & 0 & 0 \\
0 & 0.03 & 0 \\
0 & 0 & 0.03
\end{bmatrix}
\end{align}

%\begin{align}
%\orientation{\mathbf{x}}(k+1) &= \discrete{\mathbf{A}} \orientation{\mathbf{x}}(k) + \discrete{\mathbf{B}} \orientation{\mathbf{u}}(k) \\
%\mathbf{e}(k+1) &= \mathbf{e}(k) + T_s \left( \mathbf{r}_{\mathcal{O},ref}(k) - \mathbf{H}\orientation{\mathbf{x}}(k) \right) \\
%\mathbf{H} &= \begin{bmatrix}
%\mathbf{0}_{3\times4} & \mathbf{I}_3 & \mathbf{0}_{3\times3} \\
%J &= \sum_{k=0}^\infty  x(k)^T\mathbf{Q}x(k) + u(k)^T\mathbf{R}u(k)
%\end{bmatrix}
%\end{align}

The proposed LQRI controller has been implemented in MATLAB. The controller gain matrix $\mathbf{K}$ is found using the function \texttt{lqi}, which takes as arguments the system equations and weighting matrices and solves the discrete algebraic Riccati equation (DARE). The gain needs to be established only once in the initialization. While the controller is running, the control input is found via
\begin{align}
u(k) = \mathbf{K} x(k).
\end{align}

Since the position and velocity state estimations and reference position come in world-coordinates from the sensor fusion, these have to be transformed to orientation-fixed frame first, before applying the control law. Also the control input, which is according to the formulation \ref{eq:lqri_u} relative to the hovering input, needs to be transformed, before returning it to the system. Algorithm \ref{alg:lqri} illustrates the described simulation.
\input{algorithms/lqri_control.tex}
\subsection{Simulation Results}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% MPC
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{MPC} 
Model predictive control is an effective control technique to deal with multivariable constraint control problems. Instead of solving an infinite horizon optimal control problem like LQR, MPC solves a finite horizon problem in a repeated fashion. At every sampling time step $k$ a $N$--step open-loop optimal control problem is solved. The solution of the first step is then applied as a control input $u(k)$ and the rest is dismissed. At the next time step $k+1$ the problem is initialized with the current state $x(k+1)$ and the open-loop optimal control procedure is repeated to find $u(k+1)$. This iterative control technique is also referred to as receding horizon control (RHC). An introduction on the topic and theory can be found in Morari's and Lee's essay \cite{Morari1999} or Maciejowski's book \cite{Maciejowski2002}.

The main advantage of model predictive control is the incorporation of constraints on inputs and states. This allows you to push the system to the limits and thus a fast control. Second, due to its predictive nature, the controller has automatic feed-forward behavior. Given the problem is convex, e.g. linear system, quadratic cost and box constraints, very efficient solvers are available and the solution is optimal with respect to the formulation. Additionally linear MPC has some advances in theory, like stability guarantees and offset-free control.

\subsection{Problem Formulation}
We will present an offset-free, reference tracking, linear MPC scheme. Steady-state offset-free control for constant references is achieved by augmenting the state dynamics with a constant disturbance as proposed by Pannocchia and Rawlings and extended by Maeder \cite{Pannocchia2003a,Maeder2009}. The disturbance is estimated using a Luenberger Observer. Reference tracking is done by finding steady-state inputs and states, that are sufficient to meet the control target. The optimization will then try to find a control input to minimize the distance between the states and inputs and their corresponding steady-state value. We choose a quadratic cost function. Note that all systems in this section are discrete time. 

Augmenting the system with input and ouput disturbances $d_k$ and tracked measurement outputs $z_k$ we obtain an extended state-space model. The dimensions are $x\in\mathbb{R}^{n_x}$, $u\in\mathbb{R}^{n_u}$, $d\in\mathbb{R}^{n_d}$, $y\in\mathbb{R}^{n_y}$ and $z\in\mathbb{R}^{n_z}$.
\begin{align}
x_{k+1} &= A x_k + B u_k + B_w w_k + B_d d_k \label{eq:MPC_dt}\\
d_{k+1} &= d_k \nonumber \\ 
y_{k} &= C x_k + C_d d_k \nonumber \\
z_k &= H y_k 
\end{align}

For our multirotor the open-loop optimal control problem formulates as following:

\begin{align}
&\min_{u(\cdot),x(\cdot)}
& & ||x_N-\bar{x}_t||_P^2 + \sum_{k=0}^{N-1} ||x_k - \bar{x}_t||_Q^2 + ||u_k - \bar{u}_t||_R^2  \nonumber\\
& \text{s.t.} 
& & x_{k+1} = A x_k + B u_k + B_w w_k + B_d d_k, & k = 0, \ldots, N-1 \nonumber\\
& & & w_{k+1} = w_k  & k = 0, \ldots, N-1 \nonumber\\
& & & d_{k+1} = d_k, & k = 0, \ldots, N-1 \nonumber\\
& & & x_k \in \mathcal{X}, \; u_k \in \mathcal{U}, & k = 0, \ldots, N-1 \nonumber\\
& & & x_N \in \mathcal{X}_f \nonumber\\
& & & x_0 = \hat x (t), \; d_0 = \hat d(t), \; w_0 = \hat w(t) \label{eq:mpc_opti}
\end{align}

As in the LQRI, we require $Q\succeq$, $P\succeq$ and $R\succ0$. We will choose $P$ to solve the Riccati equation. This mimics unconstrained infinite horizon control at the end of the horizon and guarantees stability for the closed-loop linear system, if the terminal set is invariant.

The steady-state targets $\bar{x}_t$ and $\bar{u}_t$ are calculated at every sampling time $t$, if possible for all $N$ future reference points. They are the solution of the following least-squares problem, which is established by solving system \ref{eq:MPC_dt} for $x_{k+1}=x_k=\bar{x}_t$, $u_k = \bar{u}_t$ and $z_k = r(t)$ with $r(t)$ being the current reference position and velocity. The inverse of the left-hand side matrix $M^{-1}$ can be obtained offline.
\begin{align}
\underbrace{\begin{bmatrix}
A-I & B \\
HC & 0 
\end{bmatrix}}_{M:=}
\begin{bmatrix}
\bar{x}_t \\
\bar{u}_t
\end{bmatrix}
=
\begin{bmatrix}
-B_w \hat{w}(t) - B_d \hat{d}(t)\\
r(t) - H C_d \hat{d}(t) \label{eq:steady_state_ls}
\end{bmatrix}
\end{align}
$\hat{w}(t)$ is the wind velocity vector estimated at the current time sample. $\hat{d}(t)$ and $\hat x (t)$ are the currently estimated disturbances and analogous estimated states. The estimator is a Luenberger observer, estimating the disturbance and state at every sample time. Instead, for example a Kalman filter could have been applied as well. The estimator is updated using the following equation. $u(t)$ is the previously calculated optimal control input. $x(t)$ is the currently measured state.
\begin{align}
\begin{bmatrix}
\hat{x}(t+1) \\ \hat{d}(t+1)
\end{bmatrix}
&=\left(
\begin{bmatrix}
A & B_d \\
0 & I
\end{bmatrix}
- L
\begin{bmatrix}
C & C_d
\end{bmatrix}
\right) \begin{bmatrix}
\hat{x}(t) \\ \hat{d}(t)
\end{bmatrix}
+
\begin{bmatrix}
B & B_w \\
0 & 0
\end{bmatrix}
\begin{bmatrix}
u(t) \\ \hat{w}(t)
\end{bmatrix}
+
L C x(t) \label{eq:mpc_obsv_update}
\end{align}

The observer gain $L$ is computed offline using regular pole placement methods for the system. Here, this is done using MATLAB's \texttt{place} command. The dual of the closed-loop feedback problem is described by the observer error dynamics \ref{eq:mpc_obsv_err} of the augmented system, which has to be observable. The poles are chosen, such that they are around ten times faster than the fastest system dynamics ($\approx 60\si{\radian\per\second}$). This may make the system sensible to measurement noise. 
\begin{align}
\begin{bmatrix}
A & B_d \\
0 & I
\end{bmatrix}
- L
\begin{bmatrix}
C & C_d
\end{bmatrix} \label{eq:mpc_obsv_err}
\end{align}

For the proposed offset-free MPC to work, we pick $n_d$, the number of disturbances, equal to the number of outputs $n_y$. According to Maeder \cite{Maeder2009}, the augmented system \ref{eq:MPC_dt} is observable if and only if $(C,A)$ observable and
\begin{align}
\det \begin{bmatrix}
A-I & B_d \\ C & I 
\end{bmatrix} = det(A-I-B_dC) \neq 0 \label{eq:mpc_obsv_cond}
\end{align}

\subsection{Implementation}
The MPC can be summarized in the following steps:
\begin{enumerate}
\item initialization: compute $P$, $L$, $M^{-1}$
\item repeat at every sample time
\begin{enumerate}
\item update state and disturbance estimation \ref{eq:mpc_obsv_update} \label{enum:mpc_start}
\item calculate $N$ desired steady-state inputs and states \ref{eq:steady_state_ls}
\item solve optimization problem \ref{eq:mpc_opti}
\item apply first control input to system \label{enum:mpc_finish}
\end{enumerate}
\end{enumerate}

\subsubsection{CVXGEN}
The optimization problem \ref{eq:mpc_opti} is the computationally limiting part in MPC, as a convex optimization problem has to be solved. One can handle this problem by explicitly solving \ref{eq:mpc_opti} for various constraint and state combinations and looking up the control law, which is piecewise affine. Another solution is online optimization, which we choose.

Convex optimization problems can be solved very efficiently using the code generation software for convex optimization CVXGEN by Mattingley and and Boyd \cite{Mattingley2010}. Unlike regular parser, the hard-coded solver solve optimizations a lot faster. They exploit the fact that the problems dimensions and structure is known a priori. For example, they take care of transformations in advance and allocate memory in an initialization phase of the running program.

For code generation, one can simply write down the problem similar to \ref{eq:mpc_opti} in the CVXGEN interface. The software then automatically generates hard-coded c-code and a MATLAB MEX interface with the solver. This solver then can be called from within MATLAB using \texttt{csolve}.

The code for generating the solver is displayed in listing \ref{code:cvxgen} in the appendix. The dimensions and the prediction horizon have to be fixed. For the proposed linear system \ref{eq:lin_sys_dt}, we have $n_x=n_d=10$ states and disturbances, $n_u=3$ control inputs and $n_w=2$ wind inputs. The number of states, control inputs, prediction horizon and constraints determines the size of the optimization problem. We choose a prediction horizon of $N=20$. For larger horizons, the program becomes too large and the computation time exceeds the limits. All other parameters can be passed to the solver before calling \texttt{csolve}.

\subsubsection{System Matrices}
As for the LQRI, we consider the heading free linear system \ref{eq:lin_sys_dt}. We discretized our model (denoted by a bar) and assign it to the above defined MPC state-space system \ref{eq:MPC_dt}. Again notice, that the position and velocity in the state and the reference need to be converted into orientation-fixed frame. 
\begin{align}
A & = \discrete{\mathbf{A}} \\
B & = \discrete{\mathbf{B}} \\
B_w & = \discrete{\mathbf{B}}_w \\
C & = \discrete{\mathbf{C}} \\
x_k &= \mathbf{x}_{\mathcal{O},k} - \mathbf{x}_{\mathcal{O},ss}  \\
u_k &= \mathbf{u}_{\mathcal{O},k} - \mathbf{u}_{\mathcal{O},ss}  \\
u_k &= \mathbf{y}_{\mathcal{O},k} - \mathbf{y}_{\mathcal{O},ss}  \\
r_k &= \begin{bmatrix}
\mathbf{r}_{\mathcal{O},ref,k} \\
\mathbf{v}_{\mathcal{O},ref,k}
\end{bmatrix}
\end{align}

The MPC will have position and velocity tracking capabilities. Thus we choose the matrix $H$ accordingly.
\begin{align}
H &= \begin{bmatrix}
\mathbf{0}_{6\times4} & \mathbf{I}_6 
\end{bmatrix} 
\end{align}
\subsubsection{Weights}
Weighting the cost function has been done iteratively, simulating step experiments and adjusting the weights until a desired performance was achieved.
\subsubsection{Disturbance Model}
As Maeder states in \cite{Maeder2009}, the simplest way to achieve offset-free control is to set the number of disturbances equal the number of outputs, $n_d=n_y=10$. The disturbance matrices $B_d$ and $C_d$ then have to be chosen such that \ref{eq:mpc_obsv_cond} holds.

A common way is to model the disturbance as output disturbance with $C_d=\mathbf{I}$ and $B_d= \mathbf{0}$. However, this is only possible, if the plant has no integrators, which our plant has.

Therefore we model the disturbance as simple input disturbance.
\begin{align}
B_d = \mathbf{I}_{10} \\
C_d = \mathbf{0}_{10\times10}
\end{align}
Physically, this means, we have disturbance accelerations (forces and moments) and velocities acting on the system, which we estimate and which the controller then can compensate.
\subsubsection{Constraints}
MPC has the great advantage to consider constraints on inputs and states explicitly in the problem formulation. One could for example use this feature for obstacle avoidance.

We will only use input constraints. For one thing, we want to keep the attitude bounded, such that it does not diverge too much from the linearization point or gets into regions of singular Euler angles. And for another, we limit the thrust within the capable regions given by the motors (see equations \ref{eq:T_min} and \ref{eq:T_max}.

\begin{align}
T_{min} \leq T_{ref} \leq T_{max} \\
\si{-30}{\degree} \leq \phi_{ref} \leq \si{30}{\degree} \\
\si{-30}{\degree} \leq \theta_{ref} \leq \si{30}{\degree}
\end{align}

\section{Simulation Results}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% NMPC
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{NMPC} 
Nonlinear MPC has the same principle as linear MPC. At every time step, a finite horizon open-loop optimal control problem is solved. The first input of the optimal input is applied to the system until the next sampling instant. Then the next finite horizon open-loop optimal control problem is solved.

Like MPC, NMPC can handle constraint systems, allowing the controller to push the system to the limits. Also, NMPC is naturally feed-forwarding, as it predicts the system states into the future and finds control inputs given this knowledge. 

Unlike MPC, however, the solution of the optimal control problem is not necessarily globally optimal. While linear MPC uses the advantages of convex optimization, the nonlinear counterpart is prone to local minima. Besides, guaranteeing stability for NMPC is more difficult. 

An introduction to the topic can be found in Allgöwer and Johansen \cite{Allgower2004,Johansen2011}. More advanced strategies and future outlooks can be found in Findeisen's, Allgöwer's and Biegler's lecture notes on NMPC \cite{Findeisen2007}.

We will derive a potentially simple reference-tracking offset-free controller for the nonlinear dynamics proposed in the state equations \ref{eq:state_diff}. For guidance and code generation we used the open-source Automatic Control and Dynamic Optimization (ACADO) toolbox written by Diehl, Ferreau, Houska, Vukov and others \cite{Houska2011,Vukov2013}.

Although Fontes \cite{Fontes2001} provides a technique to guarantee closed-loop stability, our controller will have no proof on stability. The approach by Fontes relies on finding a terminal penalty and terminal region such that a Lyapunov function can be found, that guarantees stability. We instead totally rely on a range of simulations. 

\subsection{Problem Formulation}
We consider the following continuous time optimal control problem (OCP).
\begin{align}
&\min_{x(\cdot),u(\cdot)} &&\int_{t}^{t+NT_s} \left( ||x(\tau)-x_{ref}(\tau)||_Q^2 + ||u(\tau)-u_{ref}(\tau)||_R^2 \d \tau \right)  \nonumber \\
&&&+ ||x(t+NT_s)-x_{ref}(t+NT_s)||_P^2, \nonumber \\
&s.t. &&\dot{x}(\tau) = f(x(\tau),u(\tau)), \nonumber \\
&&&\dot{w}(\tau) = 0, \nonumber \\
&&&\dot{\psi}(\tau) = 0, \nonumber \\
&&&x(\tau) \in \mathcal{X}, \; u(\tau) \in \mathcal{U}, \quad \forall\tau\in\left[t,t+NT_s\right], \nonumber \\
&&&x(t) = \hat{x}(t),\; w(t) = \hat{w}(t), \; \psi(t) = \hat{\psi}(t).  \label{eq:nmpc_opt_prob}
\end{align}
$N$ is the time horizon, $T_s$ is the sampling time, $t$ is the current sampling instant time, $Q\succeq0$ is the state penalty, $R\succ0$ is the input penalty, $P\succeq0$ is the terminal state penalty, $f(\cdot)$ is the differential equation governing the system, $\mathcal{X}$ is the unconstrained state space, $\mathcal{U}$ is the constrained input space, $\hat{x}(t)$ is the state, $\hat{w}(t)$ the wind and $\hat{\psi}(t)$ the heading measurements at the current sampling instant.

In order to achieve offset-free control at steady state, the state $x(t)$ is augmented with an integrator state $e(t)$.
\subsection{Implementation}
The NMPC is implemented using MATLAB. The solver to the OPC is generated offline using the ACADO toolkit. The solver is then called at every sample instant. The running program can be summarized into the following steps:

\begin{enumerate}
\item initialization: compute $P$, set warm start
\item repeat at every sample time
\begin{enumerate}
\item set online data (wind, heading)
\item set reference
\item set warm start
\item solve OPC
\item save future inputs and outputs
\item update integrator
\item apply first control input
\end{enumerate}
\end{enumerate}
\subsubsection{ACADO}
For generating the solver we use the ACADO toolbox. The toolbox has a MATLAB interface that simplifies the creation. Vukov gives a good introduction to the solver \cite{Vukov2013}.The OCP \ref{eq:nmpc_opt_prob} is discretized using multiple shooting as presented by Bock \cite{bock1984multiple}. The discrete non-linear least square optimal control problem is then solved using a generalized Gauss-Newton method, which was elaborated by Bock as well \cite{Bock1983}. Diehl optimized this method for MPC. The real-time iteration (RTI) scheme \cite{Diehl2005} uses the control inputs and states from the previous optimization run as a new linearization point and following that only one Newton-iteration is necessary per sampling instant.

ACADO supports different solvers for the discretized non-linear least square OCP. By default, ACADO uses the dense linear algebra solver qpOASES written by Ferreau \cite{Ferreau2014}. It uses a condensing based RTI scheme, which reduces the number of optimization variables and performs well for short prediction horizons.

In MPC long prediction horizons are desired. For our problem we will have $N=100$. A sparse quadratic program solution is more efficient. Recently Frasch released a new open-source sparse solver called qpDUNES \cite{Frasch}. It uses a mix of the structure exploitation of interior point methods and warm-starting capabilities of active set methods. The solving time for the hexacopter OCP \ref{eq:MPC_dt} was cut down to a half using the sparse solver instead of the condensed solver.

Other solvers supported by ACADO are HPMPC by Frison \cite{www:hpmpc} and FORCES by Dimahidi \cite{www:forces}. They were not used because either they were not fully supported by the MATLAB interface or required a license.

Further we optimized the ACADO solver settings to achieve a computation time as small as possible (see chapter). We use a second order implicit Runge-Kutta method for integration instead of a fourth order method, reduced the number of integrator iterations to two (5 by default) and used $N=100$ integrator steps.

The code for creating the solver is printed in listing \ref{code:acado}.
\subsubsection{System Dynamics}
We can directly use the derived non-linear system dynamics \ref{eq:state_diff}. We will consider the positions and translational velocities in world-fixed coordinates and the angles and angular rates in orientation-fixed coordinates. The state however, is augmented by an integrator.
\begin{align}
x &= \begin{bmatrix}
\mathbf{x} \\
\mathbf{e}
\end{bmatrix} \\
u &= \mathbf{u} \\
w &= \mathbf{w} \\
f(x(t),u(t)) &= \begin{bmatrix}
f(\mathbf{x},\mathbf{u},\mathbf{w}) \\
\ddt \mathbf{e}
\end{bmatrix} \label{eq:ocp_dyn}
\end{align}
The dynamics of the wind and the heading are actually not modeled as in the OCP \ref{eq:nmpc_opt_prob}, but rather fed to the NMPC as a constant $N\times4$ array. This can be done in ACADO using the \texttt{OnlineData} data type.
\subsubsection{Integrator}
The integrator follows the dynamics
\begin{align}
\ddt \mathbf{e}_{real}(\tau) &= \mathbf{r}_{ref} - \mathbf{r(\tau)},  \label{eq:e_dt} \\
e_{real}(t) &= e_{real}(t), \\
-e_{max} \mathbf{1} &\leq e_{real}(t) \leq e_{max}  \mathbf{1} .
\end{align}
It is limited by the upper and lower value $e_{max}=0.2$ in order to prevent the system from wind-up. However, this constraint can not straight forward be implemented in the ACADO differential equation. 

If it was just set as a constraint, the problem immediately goes infeasible, if the set point is to far away from the current position. Imagine for example the MAV being away $x_{ref}-x(t) = 1 \si{\metre}$ away from the reference position. If the MAV can not make it to the reference within the prediction horizon, the integral term will strictly monotonously grow, exceed the maximum value, and the optimization problem is infeasible.

To prevent this, we do not constraint the error term hard, but rather window its differential equation. If the error term is outside the boundary, the growth will be set to zero. We impose a rectangular window on the error function \ref{eq:e_dt}. Because a rectangular window function is discontinuous, we will rather approximate the step functions generating the window by a combination of logistic functions \ref{eq:logistics_window}. Such a window is depicted in figure \ref{fig:logistics_window}.
\begin{align}
W(e_x) &= \frac{L}{1+\exp{(-k(e_x+e_{max}))}}  - \frac{L}{1+\exp{(-k(e_x-e_{max}))}}\label{eq:logistics_window}
\end{align}
\begin{figure}
\centering
\includegraphics{images/window.tikz}
\caption{Approximated rectangular window as combination of logistics functions with $e_{max}=0.2$, $L=1$, $k=10^2$}
\label{fig:logistics_window}
\end{figure}
Thus, the integrator dynamics in \ref{eq:ocp_dyn} are modeled for the non-linear OCP:
\begin{align}
\ddt \mathbf{e}(\tau) &= \begin{bmatrix} 
W(e_x(\tau)) & 0 & 0 \\
0 & W(e_y(\tau)) & 0 \\
0 &  0 & W(e_z(\tau))
\end{bmatrix}
\left(\mathbf{r}_{ref} - \mathbf{r(\tau)} \right), \\
\mathbf{e}(t) = \mathbf{e}_{real}(t).
\end{align}
\subsubsection{Reference}
The reference $x_{ref}$ inputted into the acado solver consists of an $N+1\times13$ array which is updated at every sample instant with the very next desired position and if possible all next $N$ future reference positions and velocities. With the controller having knowledge about the desired future position and velocity, feedforward is granted. If they are not known, the position reference will just be constant over the whole prediction horizon and the velocity reference will be set to zero. 

The reference attitude is set to zero, which causes some "damping", depending on the weight on the attitude error term. The integrator reference is set to zero, to make the system drive the integrator to zero.
\begin{align}
x_{ref} = \begin{bmatrix}
0 & 0 & 0 & 0 & \mathbf{r}_{ref,t} & \mathbf{v}_{ref,t} & 0 & 0 & 0 \\
0 & 0 & 0 & 0 & \mathbf{r}_{ref,t+T_s} & \mathbf{v}_{ref,t+T_s} & 0 & 0 & 0 \\
 &  &  &  & \vdots &  &  &  &  \\
0 & 0 & 0 & 0 & \mathbf{r}_{ref,t+NT_s} & \mathbf{v}_{ref,t+NT_s} & 0 & 0 & 0 
\end{bmatrix}
\end{align}

The reference input $u_{ref}$ consists of a $N\times3$ array of the steady state hovering inputs. With according penalty on the offset from hovering, this leads to some damping as well.
\begin{align}
u_{ref} = \begin{bmatrix}
mg & 0 & 0 \\
mg & 0 & 0 \\
  & \vdots &  \\
mg & 0 & 0 
\end{bmatrix}
\end{align}
\subsubsection{Weights}
The final penalty $P$ is the solution of the continuous time algebraic Riccati equation \ref{eq:care} of the linearized system \ref{eq:lin_sys_dt}. The solution is found using MATLAB's \texttt{lqi}.
\begin{align}
A^TP+PA-PBB^TP+Q=0\label{eq:care}
\end{align}
While this is not totally correct, as the linearized system uses an orientation-fixed coordinate system for the translational dynamics, it still gives an approximation for the infinite horizon OCP solution at the final time step.
\subsubsection{Constraints}
Again, we will only use input constraints. We want to keep the attitude bounded, such that it does not diverge too much from the linearization point (our attitude dynamics are still linearized around hovering) or even gets into regions of singular Euler angles. And we limit the thrust within the capable regions given by the motors (see equations \ref{eq:T_min} and \ref{eq:T_max}. However, we allow a little more tilt than in the linear MPC.

\begin{align}
T_{min} \leq T_{ref} \leq T_{max} \\
\si{-45}{\degree} \leq \phi_{ref} \leq \si{45}{\degree} \\
\si{-45}{\degree} \leq \theta_{ref} \leq \si{45}{\degree}
\end{align}

\subsubsection{Initialization and Warm Start}
At every sample instant we will initialize the OCP \ref{eq:nmpc_opt_prob} with the previously determined optimal state and input. This will lead to convergence of the NMPC loop according to Diehl's work on the real-time iteration (RTI) scheme \cite{Diehl2005}.

At the very first sampling instant, the OCP state vector is initialized with zero attitude and the desired position and velocity. The optimal input is initialized with hovering.

\subsection{Simulation Results}